# Chapter 3: Logistic regression 

*In this chapter, we will practice logistic regression and model validation with example data.*

We will start by reading in the data that we will be using in this exercise. The data consists of 370 rows and 33 columns. 


```{r}
# set working directory 
setwd("C:/Users/koivusus/IODS/IODS-project_new")

# read in data 
data <- read.csv("data/alc.csv") 

# print out the column names 
colnames(data)

# check dimensions
dim(data)
```

My aim is to investigate which variables explain the high/low alcohol consumption in students. More specifically, I am interested in whether students gender (sex), student's home address type (address), family educational support (famsup) and attended nursery scool (nursery) explain the high/low alcohol consumption. 

I hypothesize that it is more likely that a student belongs to the group of high alcohol consumption if 
1) the student is male (males may be more prone to use high alcohol doses), 
2) the student lives in a rural area (in rural areas high alcohol consumption may be more common than in urban areas because e.g. other substances may play a bigger part), 
3) the student has lower family educational support (lower family educational support may cause students e.g. to feel depressed), 
4) the student has not attended nursery school (not as much knowledge about the health effects of alcohol).

Next, we will explore the distributions of the chosen variables and their relationships with alcohol consumption. Let's first produce a barplot of each variable. Looks like they are all binary. 

```{r}

# load library
library(ggplot2)
library(tidyr)
library(dplyr)

# draw a bar plot of each variable
data %>% select(sex, address, famsup, nursery) %>% gather() %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar()

```

Let's do some cross-tabulations. As I hypothesized, the percent of high alcohol consumption students higher in men than in women. The percent of high alcohol consumption students is also higher in students living in rural areas, in students with no family educational support and within students that have not attended nursery school. All results so far therefore support my hypotheses. 


```{r}

# see how many percent in men and in women belong to the high/low alcohol consumption class
data %>% group_by(sex, high_use) %>% summarise(count = n()) %>% mutate(prop = count / sum(count))

# see how many percent in students living in urban/rural areas belong to the high/low alcohol consumption class
data %>% group_by(address, high_use) %>% summarise(count = n()) %>% mutate(prop = count / sum(count))

# see how many percent in students that have received family educational support belong to the high/low alcohol consumption class
data %>% group_by(famsup, high_use) %>% summarise(count = n()) %>% mutate(prop = count / sum(count))

# see how many percent in students that have or have not attended nursery education belong to the high/low alcohol consumption class
data %>% group_by(nursery, high_use) %>% summarise(count = n()) %>% mutate(prop = count / sum(count))


```

To further analyze the relationship between the chosen variables, I will run a logistic regression model. 

The summary of the model shows that only variables "sex" and "address" had statistical relationships with alcohol consumption, with the p-value of address being only 0.09. The coefficient of 0.916651 for the "sexM" variable suggests that being male (sexM) is associated with an increase in the log-odds of the response variable being TRUE (high_use) by 0.916651. The coefficient of -0.459666 for the "addressU" variable suggests that having an address of "U" (Urban area) is associated with a decrease in the log-odds of the response variable being TRUE (high_use) by 0.459666. 

Next, we will look closer into the odds ratios, which quantify the strength of the relationship between dependent and independent variables. The results can be interpreted so that if OR = 1, the odds of high_use being TRUE or FALSE are the same, regardless of the dependent variable. On other words, odds ratio values that are other than 1 have some kind of effect on the response variable. Thus, for all other variables than sex, the condifence interval includes the value 1, suggesting that only sex has statistically significant effects on the odds of high_use being TRUE or FALSE. 


```{r}
# find a model
mod1 <- glm(high_use ~ sex + address + famsup + nursery, data = data, family = "binomial")

# print summary 
summary(mod1)

# compute odds ratios (OR)
OR <- coef(mod1) %>% exp

# compute confidence intervals (CI)
CI <- confint(mod1) %>% exp

# print out the odds ratios with their confidence intervals
cbind(OR, CI)

```

Let's fit the model again with the variable that had statistically significant effect on alcohol consumption (sex) and investigate the predictive power of the model. 

According to the cross tabulation of predictions versus actual values, rows that had actual value FALSE were predicted to be FALSE by the model 259 times (70 %), whereas actual values TRUE were predicted as FALSE 111 times (30 %). In other words, the model predicted FALSE for all actual values. Thus, the model maybe works a bit bettew than simply guessing in which class a person belongs. 

```{r}
# fit a model with only the variables that had statistical relationship with alcohol consumption 
mod2 <- glm(high_use ~ sex, data = data, family = "binomial")

# predict() the probability of high_use
probabilities <- predict(mod2, type = "response")

# add the predicted probabilities to 'alc'
data <- mutate(data, probability = probabilities)

# use the probabilities to make a prediction of high_use
data <- mutate(data, prediction = probability > 0.5)

# see the last ten original classes, predicted probabilities, and class predictions
select(data, sex, high_use, probability, prediction) %>% tail(10)

# tabulate the target variable versus the predictions
table(high_use = data$high_use, prediction = data$prediction) # prediction produced FALSE to all study persons 

# print the table in percentages 
prop.table(table(high_use = data$high_use, prediction = data$prediction)) * 100
```
Let's perform 10-fold cross-validation. The model slightly higher prediction error than the one in the exercise (circa 0.31 versus 0.26). 

```{r}
# define a loss function (mean prediction error)
loss_func <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5 
  mean(n_wrong)
}

# load library
library(boot)

# K-fold cross-validation
cv <- cv.glm(data = data, cost = loss_func, glmfit = mod2, K = 10)

# average number of wrong predictions in the cross validation
cv$delta[1]
```

*Super-Bonus: Perform cross-validation to compare the performance of different logistic regression models (= different sets of predictors). Start with a very high number of predictors and explore the changes in the training and testing errors as you move to models with less predictors. Draw a graph displaying the trends of both training and testing errors by the number of predictors in the model.*


```{r}

```
# Chapter 5: Dimensionality reduction techniques

*In this chapter, we will practice dimensionality reduction techniques with example data.*

Let's start by reading in the data. The data consists of 9 variables of which only exp_educ is relatively normally distributed. The rest of the variables are skewed in one way or another. The strongest correlation is between exp_life and exp_educ, indicating that in countries where more people have a secondary education, the life expectancy is higher.

```{r}
# read in the data 
human <- read.csv("C:/Users/koivusus/IODS/IODS-project_new/data/human.csv")

# inspect the data 
head(human)

# remove X column 
human <- dplyr::select(human, -X)

# move the country names to rownames 
library(tibble)
human1 <- column_to_rownames(human, "Country")

# show a graphical overview 
library(GGally) 
ggpairs(human1, progress = FALSE)

# show summaries 
summary(human1)
```

Let's perform a principal component analysis (PCA) on the raw (non-standardized) human data, and show the variability captured by the principal components in percentages. 

Let's draw a biplot displaying the observations by the first two principal components (PC1 coordinate in x-axis, PC2 coordinate in y-axis), along with arrows representing the original variables.

```{r}
# perform principal component analysis (with the SVD method)
pca_human1 <- prcomp(human1)

# save summary in an object 
s <- summary(pca_human1)

# show variability captured by the principal components as rounded percentanges of variance captured by each PC
pca_pr <- round(1*s$importance[2, ], digits = 5)

# print out the percentages of variance
print(pca_pr)

# create object pc_lab to be used as axis labels
pc_lab <- paste0(names(pca_pr), " (", pca_pr, "%)")

# draw a biplot
biplot(pca_human1, cex = c(0.8, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2])
```

Let's standardize the variables in the human data and repeat the analysis. 

Looks like the results from raw data vs. standardized data differ from each other quite a lot. In the raw data, the first component takes almost all of the variation, whereas in the standardized data, the variation is split between multiple components, with the first one covering c. 54 % of variation. 

By scaling the variables to have a mean of 0 and a standard deviation of 1, we make sure that all variables are on the same scale. This allows PCA to give each variable equal weight and prevents variables with larger magnitudes (like GII in this case) from dominating the analysis. In other words, scaling the data makes the  variables "comparable" to each other. 

When two vectors in the biplot are close, forming a small angle, the two variables are positively correlated (e.g. mort_ratio and birth_rate). If they meet each other at 90°, they are not likely to be correlated (e.g.labo_ratio and exp_life). When they diverge and form a large angle (close to 180°), they are negative correlated (e.g.labo_ratio and birth_rate). 

The variables that are grouped to the component 1 are 

- Maternal Mortality, 
- Life Expectancy, 
- Education Expectancy, 
- Adolescent Birth Rate, 

whereas variables grouped to the component 2 are 

- Labour Force Participation Rate female / male ratio, 
- Percent Representation in Parliament (Female). 

Thus, the PCA seems to group variables related to individuals life (PC1) and variables related to society (PC2).


```{r}
# standardize the variables
human_std <- scale(human1)

# perform principal component analysis (with the SVD method)
pca_human_std <- prcomp(human_std)

# save summary in an object 
s_std <- summary(pca_human_std)

# show variability captured by the principal components as rounded percentanges of variance captured by each PC
pca_pr_std <- round(1*s_std$importance[2, ], digits = 5)

# print out the percentages of variance
print(pca_pr_std)

# create object pc_lab to be used as axis labels
pc_lab_std <- paste0(names(pca_pr_std), " (", pca_pr_std, "%)")

# draw a biplot
biplot(pca_human_std, cex = c(0.8, 1), col = c("grey40", "deeppink2"), xlab = pc_lab_std[1], ylab = pc_lab_std[2])

```

Let's download the tea data from the FactoMineR package and convert its character variables to factors. It is a questionnaire on tea: 300 individuals were asked how they drink tea and what are their product's perception. In addition, some personal details were asked.

```{r}
# load the tea dataset and convert its character variables to factors 
tea <- read.csv("https://raw.githubusercontent.com/KimmoVehkalahti/Helsinki-Open-Data-Science/master/datasets/tea.csv", stringsAsFactors = TRUE)

```

Let's explore the data briefly by looking at the structure and the dimensions of the data, viewing its contents and visualizing it. 

```{r}
# explore structure 
str(tea)

# explore dimensions
dim(tea)

# view the data
View(tea)

# load libraries 
library(dplyr)
library(tidyr)

# select columns to plot (otherwise there are too many to show in one plot)
tea_time <- dplyr::select(tea, Tea, How, how, sugar, where, lunch)

# visualize the dataset
library(ggplot2)
pivot_longer(tea_time, cols = everything()) %>% 
  ggplot(aes(value)) + geom_bar() + facet_wrap("name", scales = "free") + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
```

Let's use Multiple Correspondence Analysis (MCA) on the tea data. Based on the visualization of the MCA, Dimension 1 seems to be group variables on how the tea is drinked whereas the Dimension 2 seems to group variables related to where the tea is drinked. 


```{r}
# multiple correspondence analysis
library(FactoMineR)
mca <- MCA(tea_time, graph = FALSE)

# summary of the model
summary(mca)

# visualize MCA
plot(mca, invisible=c("ind"), graph.type = "classic", habillage = "quali")
```

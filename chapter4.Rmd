# Chapter 4: Clustering and classification 

*In this chapter, we will practice clustering and classification with example data.*

*Load the Boston data from the MASS package. Explore the structure and the dimensions of the data and describe the dataset briefly, assuming the reader has no previous knowledge of it. Details about the Boston dataset can be seen for example here. (0-1 points)*

We will start by reading in the "Boston" data from the MASS package. The data consists of 506 rows and 14 columns. It has both numeric and integer variables. 

```{r}
# load library
library(MASS)

# load the data
data("Boston")

# explore the dataset
str(Boston)
dim(Boston)
```

*Show a graphical overview of the data and show summaries of the variables in the data. Describe and interpret the outputs, commenting on the distributions of the variables and the relationships between them. (0-2 points)*

```{r}
# load library
library(corrplot)

# show a graphical overview of the data 
pairs(Boston)

# make a nicer correlation plot 
cor_matrix <- cor(Boston) %>% round(2)
corrplot(cor_matrix, method = "circle", type = "upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)

# show summaries of the variables 
summary(Boston)

```

*Standardize the dataset and print out summaries of the scaled data. How did the variables change? Create a categorical variable of the crime rate in the Boston dataset (from the scaled crime rate). Use the quantiles as the break points in the categorical variable. Drop the old crime rate variable from the dataset. Divide the dataset to train and test sets, so that 80% of the data belongs to the train set. (0-2 points)*

```{r}
# center and standardize variables
boston_scaled <- scale(Boston)

# print summaries of the scaled variables
summary(boston_scaled) # note that now all means are 0 as supposed to 

# change the object to data frame
boston_scaled_df <- as.data.frame(boston_scaled)

# create a quantile vector of crim
bins <- quantile(boston_scaled_df$crim)

# create a categorical variable 'crime'
crime <- cut(boston_scaled_df$crim, breaks = bins, include.lowest = TRUE, label = c("low", "med_low", "med_high", "high"))

# remove original crim from the dataset
boston_scaled_df <- dplyr::select(boston_scaled_df, -crim)

# add the new categorical value to scaled data
boston_scaled_df <- data.frame(boston_scaled_df, crime)

# number of rows in the Boston dataset 
n <- nrow(boston_scaled_df)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled_df[ind,]

# create test set 
test <- boston_scaled_df[-ind,]

```

*Fit the linear discriminant analysis on the train set. Use the categorical crime rate as the target variable and all the other variables in the dataset as predictor variables. Draw the LDA (bi)plot. (0-3 points)*

```{r}
# fit a linear discriminant analysis
lda.fit <- lda(crime ~ . , data = train)

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  graphics::arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results (select both lines and execute them at the same time!)
plot(lda.fit, dimen = 2)
lda.arrows(lda.fit, myscale = 1)
```

*Save the crime categories from the test set and then remove the categorical crime variable from the test dataset. Then predict the classes with the LDA model on the test data. Cross tabulate the results with the crime categories from the test set. Comment on the results. (0-3 points)*

```{r}
# save the classes from test data
classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = classes, predicted = lda.pred$class)
```

*Reload the Boston dataset and standardize the dataset (we did not do this in the Exercise Set, but you should scale the variables to get comparable distances). Calculate the distances between the observations. Run k-means algorithm on the dataset. Investigate what is the optimal number of clusters and run the algorithm again. Visualize the clusters (for example with the pairs() or ggpairs() functions, where the clusters are separated with colors) and interpret the results. (0-4 points)*

*When you plot the number of clusters and the total WCSS, the optimal number of clusters is when the total WCSS drops radically.*

```{r}
# load the data
data("Boston")

# scale the Boston dataset 
boston_scaled <- scale(Boston)

# calculate euclidean distances
dist_eu <- dist(boston_scaled)

# look at the summary of the distances
summary(dist_eu)

# set seed to get reproducable results 
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line') # the optimal number of clusters seems to be around 2

# k-means clustering
km <- kmeans(boston_scaled, centers = 2)

# plot the Boston dataset with clusters
pairs(boston_scaled, col = km$cluster)

```

*Bonus: Perform k-means on the original Boston data with some reasonable number of clusters (> 2). Remember to standardize the dataset. Then perform LDA using the clusters as target classes. Include all the variables in the Boston data in the LDA model. Visualize the results with a biplot (include arrows representing the relationships of the original variables to the LDA solution). Interpret the results. Which variables are the most influential linear separators for the clusters? (0-2 points to compensate any loss of points from the above exercises)*

*Super-Bonus: Run the code below for the (scaled) train data that you used to fit the LDA. The code creates a matrix product, which is a projection of the data points.*
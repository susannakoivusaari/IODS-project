# Chapter 2: Regression and model validation

*In this chapter, we will practice regression and model validation with example data.*

We will start by reading in the data that we will be using in this exercise.

```{r}
# set working directory 
setwd("C:/Users/koivusus/IODS/IODS-project_new")

# read in data 
data <- read.csv("data/learning_2014_sel_fil.csv") 
```

Next, we will explore the dimensions and the structure of the data using dim() and str() functions. According to the results, the data consist of 166 observations of 7 variables, which are in character, integer, and numerical formats. The variable "points" tells us how many points a person got in an exam, whereas variables "deep", "stra" and "surf" give us a look into whether a person has relied on deep, surface and strategic learning. The two character variables, "gender" and "age", give us some basic information about the study persons. Finally, the variable "attitude" tells us about the attitude of the study person towards statistics.

```{r}
# explore dimensions (number of rows and columns)
dim(data)

# explore structure 
str(data)

# remove unnecessary X column 
data <- dplyr::select(data, !X)
```

Most of the variables in the data are relatively normally distributed. The clearest exception is age -- the study was clearly focused on people around their twenties. Furthermore, a larger portion of the study persons were women. Out of all the variables, attitude towards statistics seems to be most strongly correlated with exam points. The size and direction of the correlation between variables between genders is otherwise pretty equal, except for age: for men, increase in age seems to affect the exam points negatively, whereas in women the effect is non significant.

```{r}

# install (if not yet installed) and load library 
# install.packages("ggplot2")
# install.packages("GGally")
library(ggplot2)
library(GGally)

# create a plot matrix with ggpairs()
ggpairs(data, mapping = aes(col = gender, alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))

# print summaries 
summary(data)


```

Next, we choose three variables, that are the most strongly correlated with exam points and fit a linear regression model using exam points as response variable. As suspected based on the correlations we calculated earlier, attitude had statistically significant effect on exam points. Variables "stra" and "surf" however did not have statistically significant effect on exam points so let's remove them from the model. 

```{r}
# fit a model with three 
model <- lm(points ~ attitude + stra + surf, data = data)

# print out summary
summary(model)

```

The results from the final model with one explanatory variable indicate, that with a unit of increase in  attitude towards statistics, the exam points increase by 0.33 units. If a larger attitude value means more positive attitude towars statistics, the results can be interpreted so that the more positive attitude you have towards statistics, the higher points you may get from the exam. However, the multiple R-squared of the model was relatively low (0.19), indicating that only 19 % of the variation in exam points was explained by attitude. Thus, it would be important to rethink which variables may affect the exam points of a study person. In my opinion, things like how many hours a person has slept during the night before the exam could be worth adding to the model.

```{r}
# fit a model with three 
model1 <- lm(points ~ attitude, data = data)

# print out summary
summary(model1)
```

The main assumptions of a Linear regression model are 1) that the spread of the residuals is roughly the same across all values of the independent variable (i.e. homoscedasticity), and that 2) the residuals are normally distribution. In addition to this, the model results can be affected by outlier observations, wherefore it is important to assess whether there are any potential ones in the data. 

Let's check how well out model satisfies these assumptions using the plot() function. The first plot (Residuals vs. Fitted values) tells us about the homoscedasticity of the model. As there are no clear trends visible in the plot (the points are quite randomly distributed) we can assume that the variance is roughly the same across the data. The second plot (Normal QQ) tells us about the normality assumption: the data seems to follow the line nicely, and therefore the residuals can be assumed to be normally distributed. The third plot (Residuals vs. Leverage) allows us to assess any outlier observations. There are few points that lie quite far away from the horizontal and vertical lines. Therefore, it may be a good idea to try to remove these points from the data and assess the model again to see if anything changes. 

```{r}

# draw diagnostic plots using the plot() function. Choose the plots 1, 2 and 5
par(mfrow = c(2,2))
plot(model1, which = c(1,2,5))

```
